{"pages":[{"title":"柯原玉 / Tony Ke","text":"TO BE ADDED SOON","link":"/about/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"持之以恒训练打卡记录","text":"每天坚持做 100 个俯卧撑每天坚持一项室内体育运动，暂定持续一年。没有特殊的健身目的，只是为了寻找一件事情让自己每天都努力去完成，锻炼自律性。 最开始的版本是一天 100 个俯卧撑，后来咨询一下发现俯卧撑是要隔天做的，因为会对肌肉产生劳损，所以打算找多几种运动来挑战，不局限于俯卧撑 从 Jan.16, 2019 开始 到 Jan.16, 2020 结束，然后开始新的挑战 April Apr.10-18 暂停一下，这阵子身体实在不太行 Apr.3-9 又生病了，休息一下 Apr.2 俯卧撑 75 个 Apr.1 俯卧撑 75 个 March Mar.31 俯卧撑 75 个 Mar.30 俯卧撑 75 个 Mar.29 俯卧撑 75 个 Mar.28 俯卧撑 75 个 Mar.27 俯卧撑 75 个 Mar.26 俯卧撑 75 个 Mar.25 俯卧撑 75 个 Mar.24 俯卧撑 75 个 Mar.23 俯卧撑 75 个 Mar.22 俯卧撑 75 个 Mar.21 俯卧撑 75 个 Mar.20 俯卧撑 75 个 Mar.19 俯卧撑 75 个 Mar.13-18 生病了，休息一下 Mar.12 俯卧撑 75 个 Mar.11 俯卧撑 100 个 Mar.10 深蹲 75 个 Mar.9 剪蹲 75 个 Mar.8 俯卧撑 75 个 Mar.7 俯卧撑 75 个 Mar.6 俯卧撑 75 个 Mar.5 俯卧撑 75 个 Mar.4 俯卧撑 75 个 Mar.3 俯卧撑 75 个 Mar.2 俯卧撑 75 个 Mar.1 俯卧撑 75 个 February Feb.28 俯卧撑 75 个 Feb.27 俯卧撑 75 个 Feb.26 俯卧撑 75 个 Feb.25 俯卧撑 75 个 Feb.24 俯卧撑 75 个 Feb.23 俯卧撑 75 个 Feb.22 俯卧撑 75 个 Feb.21 俯卧撑 75 个 Feb.20 俯卧撑 75 个 Feb.19 剪蹲 100 个 Feb.18 俯卧撑 75 个 Feb.17 深蹲 100 个 Feb.16 仰卧起坐 100 个 Feb.15 剪蹲 100 个 Feb.14 俯卧撑 75 个 Feb.13 深蹲 100 个 Feb.12 俯卧撑 65 个 Feb.11 俯卧撑 75 个 Feb.10 俯卧撑 75 个 Feb.9 俯卧撑 75 个 Feb.8 深蹲 100 个 Feb.7 俯卧撑 75 个 Feb.6 俯卧撑 75 个 Feb.5 俯卧撑 75 个 Feb.5 俯卧撑 75 个 Feb.4 深蹲 100 个 Feb.3 剪蹲 80 个 Feb.2 俯卧撑 75 个 Feb.1 深蹲 60 个 January Jan.31 俯卧撑 50 个 Jan.30 俯卧撑 50 个 Jan.29 俯卧撑 50 个 Jan.28 剪蹲 100 个 Jan.27 俯卧撑 50 个 + 剪蹲 100 个，肩膀酸 Jan.26 仰卧起坐 100 个 Jan.25 俯卧撑 60 个 Jan.24 俯卧撑 60 个 Jan.23 俯卧撑 40 个 Jan.22 剪蹲 80 个 Jan.21 深蹲 75 个 Jan.20 俯卧撑 40 个 Jan.19 仰卧起坐 80 个 Jan.18 剪蹲 100 个 Jan.17 深蹲 80 个 Jan.16 俯卧撑 100 个","link":"/2019/01/16/365-days-challenge/"},{"title":"由 Eval 引申到 Redis Cluster 的学习","text":"背景因为业务需要写了一段 Redis 的 Lua Script，在调用 EVAL 的时候发现还有两个参数 KEYS 和 ARGV，感觉很疑惑，难道不是简单的传点参数进去就可以？所以了解了一下这两个参数的含义。 EVAL 命令的定义是1EVAL script numkeys key [key ...] arg [arg ...] All Redis commands must be analyzed before execution to determine which keys the command will operate on. In order for this to be true for EVAL, keys must be passed explicitly. This is useful in many ways, but especially to make sure Redis Cluster can forward your request to the appropriate cluster node. Redis 设计的原意是使用者应当将在 Lua Script 中操作的所有 KEY 以 KEYS 的形式传给 EVAL，这样 EVAL 能够在真正运行前检查一下 KEY，就目前来说，是根据 KEYS 来决定把指令转发到哪台 Redis Cluster Master Node 上。ARGV 则是传递一些必要的参数，这个倒是没什么使用规范。 于是我先放下 EVAL，了解一下 Redis Cluster，用了这么久 Redis 了，还没用过 Redis Cluster，惭愧。 Redis ClusterRedis Cluster 简单的来说就是若干 Redis 节点组成集群，数据根据 KEY 被分散到不同的节点上面，而不是节点之间互为 Replication。KEY 的分散策略不是 Consistent Hashing，而是一种叫做 Slots 的机制，在文章最后我再介绍一下我对 Slots 机制的理解。也正是因为 Slots 机制，指令到达 Redis Cluster 的节点之后会根据 KEY 选择合适的 Slot，然后客户端重新连接到对应的节点再执行操作，而不是某个节点转发指令到对应 KEY 的节点。一个 Redis Cluster 一共有 16384 个 Slots，最开始创建集群的时候这 16384 个 Slots 会被平均分散到各个节点上面，也可以通过命令来手动调整，至于为什么是 16384 个 Slots，感兴趣的同学可以看一下作者在这个 Issue 上的回答 Availability前面说到 Redis 节点之间数据互相独立，所以只要有节点挂了，这个节点负责的 Slots 的数据也变得无法访问从而整个集群的节点都会拒绝访问，处于 CLUSTERDOWN 的状态。因此 Redis Cluster 也利用了 Master-Slave 机制用于提供 Availability，即 Master 节点会异步复制自己的数据（其实是复制操作）到它 N 个的 Slave 节点，也就是说一个 Slot 对应的 Key 们会有 N+1 份数据分布在 Master 和 Slave 上。当某个 Master 挂了的时候，集群会把它的 Slave 升级到 Master，这样子集群就能继续工作。当然，如果一对 Master-Slave 都挂了那集群也就挂了。通常来说，所有操作都会在 Master 上进行，Slave 只是充当备胎的作用，只有 Master 挂了 Slave 才能上位。不过 Slave 也不是一无是处，客户端可以通过执行 READONLY 命令来将客户端切换到只读模式，这样子读操作就会在 Slave 上进行。 ConsistencyRedis Cluster 并不能保证很好的数据一致性，在某些极端情况下还是有可能丢数据的： 第一种丢失数据的情况Master 没有成功复制消息到 Slave。在 Master-Slave 模式下，对 Master 的操作是同步的，即操作成功后才会返回给客户端相应的消息，但是 Master -&gt; Slave 的复制是异步的，假设一种情况： 客户端 SET a 1 Master 接收到指令 SET a 1，回复给客户端 OK Master 在发送给 Slave SET a 1 之前崩了 Slave 上位成 Master这时候 a=1 这个数据就丢失了。这种情况也不是不能避免，Redis Cluster 提供了 WAIT 指令来做到这件事情。客户端发送 WAIT 指令之后，Redis Cluster 会 Block 住直到客户端的上一条操作有 N 个 Slave 都回复 ACK 了。 第二种丢失数据的情况网络隔离。假设我们有一个三节点集群，每个 Master 节点有一个 Slave，节点命名为 A-A1, B-B1, C-C1，以及一个客户端 D发生网络隔离的情况下，D 和少数 Master（C） 形成一个孤岛，其他大多数 Master 形成一个孤岛。 孤岛 AD、C 孤岛 B A、A1、B、B1、C1 t0 C 与集群失联，开始等待恢复连接等待时间为 node timeout[4] 集群与 B 失联，开始等待 B 恢复连接等待时间为 node timeout t1 D 向 C 节点写数据 持续等待 t2 C 等待超时切换状态到 error，拒绝写入新数据 集群等待超时通过选举将 C1 提升为新的 Master t3 与集群恢复联系 与 D C 恢复联系 恢复联系后，因为原来的 Master C 已经被多数成员认为不可用淘汰掉了，C1 被选为新的 Master，C 加回集群后被降级为 Slave。因为先前 D 向 C 写的数据没有同步到 C1，所以数据丢失。 回到 EVAL在一个节点接收到 EVAL 指令之后，他会检查 KEYS，算出对应的 Slots，如果所有 KEY 不是落到同一个 Slot 上，会提示 CROSSSLOT Keys in request don't hash to the same slot 那如果我不传 KEYS，直接在脚本中操作呢？还是会报错。12$ redis-cli EVAL &quot;redis.call(&apos;get&apos;, &apos;slot a&apos;); redis.call(&apos;get&apos;, &apos;slot-b&apos;)&quot; 0ERR Error running script (call to f_8ead0f68893988e15c455c0b6c8ab9982e2e707c): @user_script:1: @user_script: 1: Lua script attempted to access a non local key in a cluster node 所以 EVAL 的时候，脚本中操作的 Key 应当保证落在同一个 Slot 里面。同时 Redis 也提供了一个方法可以保证 Key 都会落到同一个 Slot 上面，下面讲 Slots 机制的时候会讲到以上关于 EVAL 的操作都是建立在对 Redis Cluster 操作的基础上的，如果使用的是单一节点，则可以不考虑这些问题，可以胡来。 Note this rule is not enforced in order to provide the user with opportunities to abuse the Redis single instance configuration, at the cost of writing scripts not compatible with Redis Cluster. Slots 机制 SLOT = CRC16(key) mod 16384 Redis 集群的拓扑结构是是一个全连通的网络，每一个节点之间都会建立一个 Cluster Bus，所以集群的任何配置变动都会立即同步到各个节点，也就是说，每一个节点都知道哪些 Slot 对应哪个节点。所以不论客户端连接到哪个节点进行执行指令，服务端都会正确的指示客户端应当重定向到哪一个节点来操作。Key 在做 CRC16 的时候，如果 Key 中存在花括号对，Redis 会使用花括号对里面字符串做 CRC16，例如 12{user:info:}1234 =&gt; crc16(&quot;user:info:&quot;) % 16384{user:info:}5737 =&gt; crc16(&quot;user:info:&quot;) % 16384 虽然是两个不同的 Key，但是花括号中间部分是一样的，所以他们有相同的 Slot。 参考资料 [1] https://github.com/antirez/redis/issues/2576 [2] https://redis.io/topics/cluster-spec [3] https://redis.io/commands/eval [4] https://redis.io/topics/cluster-tutorial","link":"/2019/04/18/a-glance-at-redis-cluster/"},{"title":"在 macOS 上构建、打包 Medis","text":"Medis 是一款简洁易用、实用的 Redis 图形化客户端，作者将这款软件在 Github 上开源，所有开发者可以自由下载它的源代码。 Medis 在 App Store 上提供了下载，价值 ¥30，算是一种形式上的捐赠吧，不愿意花钱的开发者也可以根据本文的指示来构建并打包 Medis，与从 App Store下载的版本并无区别。 前提 Xcode &gt;= 8.2.1 macOS &gt;= 10.11.6 Node 8（我只在装了 Node 8 的机器上尝试过） 自力更生123456789101112131415161718192021# 1. 下载最新源代码$ git clone https://github.com/luin/medis.gitCloning into 'medis'...remote: Counting objects: 3154, done.remote: Total 3154 (delta 0), reused 0 (delta 0), pack-reused 3154Receiving objects: 100% (3154/3154), 48.44 MiB | 1.89 MiB/s, done.Resolving deltas: 100% (1745/1745), done.$ cd medis# 2. 安装依赖$ npm install...added 1295 packages from 1824 contributors in 49.707s# 3. 打包，忽略 Unhandled rejection Error$ npm run packPackaging app for platform mas x64 using electron v1.4.15flating... ~/Desktop/medis/out/Medis-mas-x64/Medis.appUnhandled rejection Error: No identity found for signing. at ~/Desktop/medis/node_modules/electron-osx-sign/flat.js:114:35 ...(stack error info) 至此，Medis 已经打包完毕，Medis.app 存放在 Medis 源代码目录下的 out/Medis-mas-x64 目录里面，运行 Medis.app 即可启动 Medis 安装也可以在 Finder 中将 Medis.app 放入应用程序目录来安装，也可以用运行如下命令来安装： 12# 此时处于 Medis 源代码根目录下$ mv out/Medis-mas-x64/Medis.app ~/Applications Let’s enjoy Medis! Thank you luin!","link":"/2018/07/19/build-medis-on-macos/"},{"title":"Kubernetes 学习札记 - 在 Google Cloud 上搭建集群","text":"Target 搭建 Kubernetes 集群（Master 1 + Node 2） Machines Hostname Machine Type kube-master n1-standard kube-node-1 n1-standard kube-node-2 n1-standard Preparation on Master Machine 安装 Docker 和 Kubeadm 保存下面脚本为 init-k8s-master.sh 123456789101112131415161718192021#!/bin/sh# install dockerapt-get updateapt-get install -y apt-transport-https ca-certificates curl software-properties-commoncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"apt-get updateapt-get install -y docker-ce# install k8scurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" &gt;&gt; /etc/apt/sources.list.d/kubernetes.listapt-get updateapt-get install -y kubelet kubeadm kubectl# ONLY run on master# configure cgroupsed -i \"s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g\" /etc/systemd/system/kubelet.service.d/10-kubeadm.confsystemctl daemon-reloadsystemctl restart kubelet 执行上述脚本 sudo source init-k8s-master.sh 选择一个 Pod 网络组件（Pod Network Add-on） 组件有很多种，这里我选择了 Flannel，具体操作见接下来两个步骤 启动集群 Master 12# 这里的 --pod-network-cidr=10.244.0.0/16 取决于你选择的网络组件sudo kubeadm init --pod-network-cidr=10.244.0.0/16 启动了之后从结果的最后几行里面获得一串命令行，用于操作 Node 加入集群，记下来，这个命令行有用 123# 获得一串类似于这样子的命令# 10.0.0.3 是我的 Master IPkubeadm join 10.0.0.3:6443 --token 8qn683.9ft7dc6xa0re697a --discovery-token-ca-cert-hash sha256:65773046272db8297b64cbc1ce8ebc3884fa932976673fad7715c2bd8c53c6a0 设置环境变量，方便调用 kubectl 如果想让非 root 用户使用 kubectl ，执行 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 对于 root 用户，直接设置环境变量 1export KUBECONFIG=/etc/kubernetes/admin.conf 这样设置之后， kubectl 才能正常运行 安装网络组件 检查节点 123$ kubectl get nodesNAME STATUS ROLES AGE VERSIONkube-master NotReady master 1m v1.11.0 会发现 STATUS 为 NotReady ，运行 kubectl describe nodes 得到最后一行日志是 Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized ，说明网络组件还没有配置好，需要设置一下。 这里我选择了 Flannel 作为网络组件，执行 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml 再次检查节点 123$ kubectl get nodesNAME STATUS ROLES AGE VERSIONkube-master Ready master 2m v1.11.0 至此 Master 配置完毕，准备配置 Node Preparation for Node Machines 安装 Docker 和 Kubeadm 参照 Master 安装教程，删去 cgroup 配置部分 加入集群 12# 在 Master 进行 kubeadm init 的时候获得的提示kubeadm join 10.0.0.3:6443 --token 8qn683.9ft7dc6xa0re697a --discovery-token-ca-cert-hash sha256:65773046272db8297b64cbc1ce8ebc3884fa932976673fad7715c2bd8c53c6a0 检查节点状态 在 Master 上执行 kubectl get nodes 12345$ kubectl get nodesNAME STATUS ROLES AGE VERSIONkube-master Ready master 2m v1.11.0kube-node-1 Ready master 3m v1.11.0kube-node-2 Ready master 3m v1.11.0 所有 STATUS 均为 Ready，完美 资料参考： Creating a single master cluster with kubeadm","link":"/2018/07/06/learn-k8s-1/"},{"title":"SSL Handshake 被莫名其妙地 RST","text":"最近我接的外包项目甲方又要开一个新的项目，于是丢过来几台阿里云的服务器，我来负责服务器基础架构的搭建，其实也就是网关，Runtime，Redis, Syslog 那些东西，已经做得滚瓜烂熟了。这次打算自己编译 OpenResty，调教出一个高性能网关，于是弄好了 OpenResty，也顺手把甲方发过来的 SSL 证书配置到了 OpenResty 上。 配置 HTTPS 的过程跟以前一样，设置好 ssl_certificate, ssl_certificate_key,listen 443 ssl http2 这些东西，再附加一些 ssl_cipher 方面的参数，openresty -s reload 完事。 reload 完了之后我在我本地的 Chrome 上通过域名访问了 https://xxx.com (xxx.com) 为甲方域名，一开始几次访问是 ERROR_CONNECTION_RESET，我也不以为然，因为服务器上的 OpenResty 刚 Reload，以及我本机上常着的 Surge 代理有时候可能会断线，所以我以为这种情况比较常见，刷新几次就好了，见到了 Welcome To OpenResty。 晚上回家之后团队的小伙伴告诉我他访问不了这个服务器的 https，我自己试了一下确实没法访问，一直提示 ERROR_CONNECTION_RESET，偶尔成功。这时候我才开始重视起来，准备研究一下这个问题。 症状 在本机上 curl 只有极小的概率会返回正确结果，大部分时候返回 12$ curl https://xxx.comcurl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to xxx.com:443 在阿里云同一 VPC 内其他机器 curl 则必成功 无论在哪里直接 curl -k https://&lt;ip&gt; 都是可以正常访问的 排查问题过程 之前用的 SSL 证书都是使用 certbot 来自助签发 Let's Encrypt 的三个月证书，这次使用的是在阿里云买的 Encryption Everywhere DV TLS CA - G1 签发的证书，我猜测甲方发给我的证书格式有问题，但是很快否定了这个猜测，因为前面讲到刷新多几次还是能访问的，并且 Chrome 能够给出正确的证书信息。 之前没有用过自己编译的 OpenResty，于是我换了个预编译版的 nginx，也就是直接通过 yum install nginx 来安装的 nginx，问题依旧。 开了 wireshark 来抓包 可以发现这么一个过程：TCP Handshake -&gt; SSL Client Hello -&gt; Reset(Server sent) 我有幸地抓到了部分没有被 Reset 的请求， 可以看出，成功的请求的 Client Hello与被 Reset 的 Client Hello 的大小都是 583 Byte，并且我详细的对比了两个 Packet，发现除了时间戳以及必要的随机数之外并没有不一样的地方。 我查看一下了 OpenResty 的错误日志，发现有大量的 peer closed connection in SSL handshake： 1234562019/05/09 22:23:48 [info] 5182#0: *5543 peer closed connection in SSL handshake (104: Connection reset by peer) while SSL handshaking, client: &lt;ip&gt;, server: 0.0.0.0:4432019/05/09 22:23:49 [info] 5183#0: *5544 peer closed connection in SSL handshake (104: Connection reset by peer) while SSL handshaking, client: &lt;ip&gt;, server: 0.0.0.0:4432019/05/09 22:23:49 [info] 5183#0: *5545 peer closed connection in SSL handshake (104: Connection reset by peer) while SSL handshaking, client: &lt;ip&gt;, server: 0.0.0.0:4432019/05/09 22:23:50 [info] 5183#0: *5546 peer closed connection in SSL handshake (104: Connection reset by peer) while SSL handshaking, client: &lt;ip&gt;, server: 0.0.0.0:4432019/05/09 22:23:50 [info] 5182#0: *5547 peer closed connection in SSL handshake (104: Connection reset by peer) while SSL handshaking, client: &lt;ip&gt;, server: 0.0.0.0:4432019/05/09 22:23:50 [info] 5183#0: *5548 peer closed connection in SSL handshake (104: Connection reset by peer) while SSL handshaking, client: &lt;ip&gt;, server: 0.0.0.0:443 也就是说，OpenResty 认为客户端在握手过程中主动关闭了连接，但是我的抓包结果却是服务端主动 RST 了连接，这让我感到很疑惑，为何两端能得出不一样的结论。由于不是很理解 SSL 握手的机制，我猜测 OpenSSL 接管了这一过程，所以我重新编译了 OpenResty，加上了最新版的 OpenSSL，问题依旧。 至此我没辙了，只能 Google, StackOverflow 一遍一遍地搜，搜到了用直接 OpenSSL 来建立 SSL 连接的方法，于是我 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455$ openssl s_client -connect xxx.com:443CONNECTED(00000005)depth=2 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert Global Root CAverify return:1depth=1 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = Encryption Everywhere DV TLS CA - G1verify return:1depth=0 CN = &lt;Hidden Part&gt;verify return:1---Certificate chain 0 s:/CN=e.duchenggo.com i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=Encryption Everywhere DV TLS CA - G1 1 s:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=Encryption Everywhere DV TLS CA - G1 i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert Global Root CA---Server certificate&lt;Hidden Part&gt;subject=/CN=&lt;Hidden Part&gt;issuer=/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=Encryption Everywhere DV TLS CA - G1---No client certificate CA names sentServer Temp Key: ECDH, X25519, 253 bits---SSL handshake has read 3246 bytes and written 285 bytes---New, TLSv1/SSLv3, Cipher is ECDHE-RSA-CHACHA20-POLY1305Server public key is 2048 bitSecure Renegotiation IS supportedCompression: NONEExpansion: NONENo ALPN negotiatedSSL-Session: Protocol : TLSv1.2 Cipher : ECDHE-RSA-CHACHA20-POLY1305 Session-ID: E5BC4CB0FFCA949F99859E6F2B6AE6D8742C6F6723EF7388ED7AED4F81C01126 Session-ID-ctx: Master-Key: 86D3EB1512786A84CE6E0ACF43362174345399F7C32A89B725893D2791AF87EBD1F642600CCA9919D57CDCB6FF6B203B TLS session ticket lifetime hint: 300 (seconds) TLS session ticket: 0000 - 85 3a da c6 e3 ab 66 f9-7b f8 e7 cd f6 a5 da e9 .:....f.{....... 0010 - ba bc d9 15 74 cf 8d b0-56 ea 4d c9 b4 f3 b7 d1 ....t...V.M..... 0020 - 1f 47 c1 1e ac 56 13 16-ca 6c d0 b8 bd 28 15 ea .G...V...l...(.. 0030 - 59 b3 79 84 b0 40 34 a4-57 e9 d1 12 c2 b5 5e 0f Y.y..@4.W.....^. 0040 - 77 02 ff 8b 4c 23 2b 89-b6 25 61 15 af 77 7c 75 w...L#+..%a..w|u 0050 - 6c 1f 80 66 27 0b 90 c4-36 43 e0 ee 75 f9 2d 99 l..f'...6C..u.-. 0060 - a0 af d3 bf 6d 12 05 f9-13 21 46 fd 41 a8 56 da ....m....!F.A.V. 0070 - df ec 89 68 d1 71 9d 15-2e 2f fa de 89 6a a0 8e ...h.q.../...j.. 0080 - 82 45 85 ca 3b 26 a0 e8-64 a9 56 82 da cf 04 4d .E..;&amp;..d.V....M 0090 - 42 13 dc 25 17 aa 38 1e-36 0a 8f 66 b5 26 57 2c B..%..8.6..f.&amp;W, 00a0 - 70 5e ff 0b 41 eb 49 f1-b0 8b 86 fd e6 c1 36 28 p^..A.I.......6( Start Time: 1557413139 Timeout : 7200 (sec) Verify return code: 0 (ok)--- 成功建立了 SSL 连接，显示的证书也是正确的，并且每一次执行都能稳定建立连接，而不是看运气才能连接得上，我抓包了这个建立过程，发现这种 Client Hello 是不携带 SNI Extension 的，如果我手动给他附上 SNI 呢？ 12345678910111213141516171819202122232425$ openssl s_client xxx.com:443 -servername xxx.comCONNECTED(00000005)write:errno=54---no peer certificate available---No client certificate CA names sent---SSL handshake has read 0 bytes and written 0 bytes---New, (NONE), Cipher is (NONE)Secure Renegotiation IS NOT supportedCompression: NONEExpansion: NONENo ALPN negotiatedSSL-Session: Protocol : TLSv1.2 Cipher : 0000 Session-ID: Session-ID-ctx: Master-Key: Start Time: 1557413484 Timeout : 7200 (sec) Verify return code: 0 (ok)--- 果不其然，无法完成握手！ 锁定元凶经过一系列排查之后，终于发现携带了 SNI 的 SSL 握手请求大概率会被 “神秘力量” Reset。 联想到这个域名还没有来得及备案，我猜想大概就是因为没有备案所以被干扰。但是以前的未备案只是 HTTP 会被劫持流量，难道阿里云已经增强了技术开始干扰未备案域名的 HTTPS 连接了吗？ 搜索了一番发现很早之前就有人讨论过这个问题了 v2ex - 阿里云竟然也能 RESET 掉 HTTPS 链接了..? v2ex - 阿里云香港和新加坡 HTTPS 极其不稳定 至此结束调查，请甲方去对域名进行备案。 真是一个无聊的调查结果，不过也是记录一下，希望其他人遇到这个问题的时候能够搜到我的文章别再浪费时间到这个问题上","link":"/2019/05/09/wired-ssl-handshake-reset/"}],"tags":[{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"study","slug":"study","link":"/tags/study/"},{"name":"DIY","slug":"DIY","link":"/tags/DIY/"},{"name":"Medis","slug":"Medis","link":"/tags/Medis/"},{"name":"kubernetes","slug":"kubernetes","link":"/tags/kubernetes/"},{"name":"学习札记","slug":"学习札记","link":"/tags/学习札记/"},{"name":"踩坑","slug":"踩坑","link":"/tags/踩坑/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"SSL","slug":"SSL","link":"/tags/SSL/"}],"categories":[{"name":"分享","slug":"分享","link":"/categories/分享/"}]}